{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Process Module - Sentiment Analysis on the Twitter Data </center></h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This file constructs the module which we import in Projectrun.ipynb file to run our application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing pandas for dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import geopy\n",
    "import json    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as pie_plot\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "from geopy.exc import GeocoderTimedOut\n",
    "geolocator = Nominatim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import random\n",
    "def grey_color_func( font_size, position, orientation, random_state=None, **kwargs):\n",
    "    return \"hsl(0, 0%%, %d%%)\" % random.randint(3, 23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "#To use the twitter mask\n",
    "image_mask = np.array(Image.open('twittermask.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Shakthi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Shakthi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Shakthi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from collections import OrderedDict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter\n",
    "from nltk import pos_tag\n",
    "from nltk import ngrams\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Shakthi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bokeh.io import output_file, output_notebook, show\n",
    "from bokeh.models import (\n",
    "  GMapPlot, GMapOptions, ColumnDataSource, Circle, LogColorMapper, BasicTicker, ColorBar,\n",
    "    Range1d, PanTool, WheelZoomTool, BoxSelectTool,BoxZoomTool\n",
    ")\n",
    "from bokeh.models.mappers import ColorMapper, LinearColorMapper\n",
    "from bokeh.palettes import Viridis5\n",
    "from bokeh.io import export_png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the access tokens to connect to twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('auth_dict.json','r') as f:\n",
    "    twtr_auth = json.load(f)\n",
    "    \n",
    "# To make it more readable, lets store the OAuth credentials in strings first\n",
    "\n",
    "access_token =  twtr_auth['token']\n",
    "access_token_secret = twtr_auth['token_secret']\n",
    "consumer_key = twtr_auth['consumer_key']\n",
    "consumer_secret = twtr_auth['consumer_secret']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Authenticating using the previously loaded access tokens\n",
    "auth = tweepy.OAuthHandler(consumer_key,consumer_secret)\n",
    "auth.set_access_token(access_token,access_token_secret)\n",
    "api=tweepy.API(auth,wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Importing the category list file to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat_df = pd.read_csv('Categories.csv',encoding='ISO-8859-1')\n",
    "#print(category_df)\n",
    "category_df = pd.DataFrame()\n",
    "for column in cat_df.columns:\n",
    "    col_count = 0\n",
    "    column_data = cat_df[column].str.lower()\n",
    "    category_df.insert(loc=col_count,column=column,value=pd.Series(column_data))\n",
    "    col_count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to handle rate limit errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def limit_handled(cursor):\n",
    "    while True:\n",
    "        try:\n",
    "            yield cursor.next()\n",
    "        except tweepy.RateLimitError:\n",
    "            print(\"Limit Exceeds. Sleeping now for 15 mins....\")\n",
    "            time.sleep(15 * 60)\n",
    "            print(\"Trying again\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating dataframe for the input passed using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def CreateDataFrame(hashtag):\n",
    "    count=0\n",
    "    prevmaxid=0\n",
    "    location=[]\n",
    "    tweet=[]\n",
    "    data= pd.DataFrame()\n",
    "    for tweets in limit_handled(tweepy.Cursor(api.search,q=hashtag,tweet_mode='extended',lang='en').items()):\n",
    "        if (prevmaxid==0 or tweets.id<prevmaxid):\n",
    "            if (tweets.lang == \"en\"):\n",
    "                raw_text = tweets.full_text\n",
    "                textwithouthttp = re.sub(r\"http\\S+\",\"\",raw_text)\n",
    "                textwithoutat = re.sub(r\"@\\S+\",\"\",textwithouthttp)\n",
    "                textRT = re.sub(r\"RT\",\"\",textwithoutat)\n",
    "                text = re.sub(r\"#\\S+\",\"\",textRT)\n",
    "                if ((\"\".join(char for char in text if ord(char)< 128)) not in tweet):\n",
    "                    tweet.append(\"\".join(char for char in text if ord(char) < 128))\n",
    "                    #tweet.append( tweets.full_text)\n",
    "                    prevmaxid=tweets.id\n",
    "                    location.append( tweets.user.location)\n",
    "                    count+=1\n",
    "                    if count == 2000:\n",
    "                        break\n",
    "    data.insert(loc = 0,column=\"tweets\",value= pd.Series(tweet))\n",
    "    data.insert(loc = 1,column=\"location\",value= pd.Series(location))\n",
    "    data.to_csv(hashtag+'.csv',index=False)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to load csv files of already collected tweets and related data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LoadFromCsv(hashtag):\n",
    "    dframe= pd.read_csv(hashtag+'.csv', engine=\"python\")\n",
    "    dframe.dropna(subset=['location'], how='all', inplace = True)\n",
    "    dframe.reset_index(drop=True, inplace = True)\n",
    "    dframe.dropna(subset=['tweets'], how='all', inplace = True)\n",
    "    dframe.reset_index(drop=True, inplace = True)\n",
    "    return dframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to perform sentiment analysis on the loaded dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SentimentAnalysis(dfnoloc):    \n",
    "    df = copy.deepcopy(dfnoloc)\n",
    "    senti_df = df['tweets'].reset_index(drop=True)\n",
    "    senti_df = pd.DataFrame(senti_df,columns=['tweets']).dropna()\n",
    "    senti_df['category'] = np.nan\n",
    "    senti_df = senti_df.reset_index(drop=True) \n",
    "    analyseSentiment = SentimentIntensityAnalyzer()\n",
    "    for itr in range(0,len(senti_df)):\n",
    "        index = list(np.where(senti_df['tweets'] == senti_df.iloc[itr]['tweets'] )[0])\n",
    "        senti_score = analyseSentiment.polarity_scores(senti_df.iloc[itr]['tweets'])\n",
    "        #classify pos or neg category based on the compound score\n",
    "        if (senti_score['compound'] > 0.0):\n",
    "            senti_df.loc[index,'category'] = 'positive'\n",
    "        else:\n",
    "            senti_df.loc[index,'category'] = 'negative'        \n",
    "    #writeToCsv(screen_name + '_Sentiment',senti_df)\n",
    "    return(senti_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merger function to join the dataframe containing locations with the catogerised data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MergeDf(dfnoloc,sentiment_df):\n",
    "    dfnoloc['category'] = sentiment_df['category'].tolist()\n",
    "    dfwithcategories=dfnoloc.drop_duplicates(subset=['location'],keep='first')\n",
    "    dfwithcategories.reset_index(drop=True, inplace = True)\n",
    "    return dfwithcategories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to find the latitude and longitude based on the locations of tweets as by default lot of tweets lack this information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Also removing duplicates to comply with geopy library restriction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def FindLocation(mergeddf_noloc):\n",
    "    count=0\n",
    "    lat=[]\n",
    "    lon=[]\n",
    "    for index,row in mergeddf_noloc.iterrows():\n",
    "        if(count<300 ):\n",
    "            try:\n",
    "                location = geolocator.geocode(row['location'],timeout=30)\n",
    "                (a,b)=(location.latitude, location.longitude)\n",
    "                lat.append(a)\n",
    "                lon.append(b)\n",
    "            except AttributeError:\n",
    "                #print(\"NA\")\n",
    "                lat.append(\"NA\")\n",
    "                lon.append(\"NA\")\n",
    "            except:\n",
    "                return (lat,lon)\n",
    "        count+=1\n",
    "    return (lat,lon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to update the locations based on longitude and latitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def UpdateLocation(mergeddf_noloc,coordinates):\n",
    "    (lat,lon)=coordinates\n",
    "    tempdf= copy.deepcopy(mergeddf_noloc.iloc[0:len(lat)])\n",
    "    tempdf['latitude'] = lat\n",
    "    tempdf['longitude'] = lon\n",
    "    return tempdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to run all the functions regarding data collection and do all the processing in single functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dataprocess(hashtag):\n",
    "    createdata= CreateDataFrame(hashtag)\n",
    "    if(createdata==True):\n",
    "        dfnoloc=LoadFromCsv(hashtag)\n",
    "        print(\"loaded data set for sentiment analysis\")\n",
    "        sentiment_df = SentimentAnalysis(dfnoloc)\n",
    "        print(\"Sentiment analysis completed successfully\")\n",
    "        mergeddf_noloc = MergeDf(dfnoloc,sentiment_df)\n",
    "        coordinates=FindLocation(mergeddf_noloc)\n",
    "        dfwithloc=UpdateLocation(mergeddf_noloc,coordinates)\n",
    "        print(\"Dataset updated successfully with classification and coordinates\")\n",
    "        dfwithloc.to_csv(hashtag+'withloc.csv',index=False)\n",
    "        print(\"data processing completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to generate the results of the Sentiment Analysis using the previously saved datafiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Analysis(filename):\n",
    "    dfwithloc= pd.read_csv(filename+'.csv', engine=\"python\")\n",
    "    wholestring=getWholeString(dfwithloc)\n",
    "    freqcounts=getFrequencies(wholestring)\n",
    "    freqTuple=Classification(freqcounts)\n",
    "    percentdf=categoryWisePercent(freqTuple)\n",
    "    plot_bargraph(percentdf,filename)\n",
    "    plot_scatterplot(dfwithloc,filename)\n",
    "    plot_wordcloud(wholestring,filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function that takes tweets as input and converts it to one continous string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It then generate tokens by removing the stopwords and filtering out the punctutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getWholeString(senti_df):\n",
    "    dataFrame = copy.deepcopy(senti_df)\n",
    "    all_nouns_text=[]\n",
    "    positive_text=[]\n",
    "    all_text=[]\n",
    "    # set the stop words\n",
    "    stop_words = set(stopwords.words('English'))\n",
    "    #set the stemmer\n",
    "    #stemmer = PorterStemmer()\n",
    "    for itr in dataFrame['tweets']:\n",
    "        #tokenize\n",
    "        tokens = word_tokenize(itr)\n",
    "        #remove stop words & punctuation(other characters)\n",
    "        filtered_text = [ word.lower() for word in tokens if (word not in stop_words) and (word.isalpha()) and (len(word) > 1)]\n",
    "       \n",
    "        # get list of all tokens\n",
    "        all_text+=filtered_text\n",
    "    \n",
    "    # take only the nouns\n",
    "    tags = pos_tag(all_text)\n",
    "    for word,pos in tags:\n",
    "            if pos in ['NN','NNS','NNP','NNPS']:\n",
    "                all_nouns_text.append(word)\n",
    "    \n",
    "    return all_nouns_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to categorize the nouns text generated from previous function and getting the frequency of the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getFrequencies(all_nouns_text):\n",
    "    nouns_text = copy.deepcopy(all_nouns_text)\n",
    "    topCount_df=pd.DataFrame()\n",
    "    all_single=[]\n",
    "    all_bigrams=[]\n",
    "    all_trigrams=[]\n",
    "    freq_single = Counter(nouns_text)\n",
    "    for token,count in freq_single.most_common(50):\n",
    "        all_single.append(token)\n",
    "    bigrams = list(ngrams(nouns_text,2))\n",
    "    freq_bi = Counter(bigrams)\n",
    "    for token,count in freq_bi.most_common(50):\n",
    "        all_bigrams.append(list(token))\n",
    "    trigrams = list(ngrams(nouns_text,3))\n",
    "    freq_tri = Counter(trigrams)\n",
    "    for token,count in freq_tri.most_common(50):\n",
    "        all_trigrams.append(list(token))\n",
    "    topCount_df.insert(loc=0,column='Single',value=pd.Series(all_single))\n",
    "    topCount_df.insert(loc=1,column='Bigrams',value=pd.Series(all_bigrams))\n",
    "    topCount_df.insert(loc=2,column='Trigrams',value=pd.Series(all_trigrams))\n",
    "    return topCount_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to get the token/word count for wordcloud generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This function also catogerizes the words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Classification(topCount_df):\n",
    "    count_df = copy.deepcopy(topCount_df)\n",
    "    # create dictionaries for tracking word count & categories\n",
    "    word_count_dict = {}\n",
    "    word_category_dict = {}\n",
    "    for row_itr in range(0,len(count_df)):\n",
    "        for column_itr in range(0,len(count_df.columns)):\n",
    "            key = count_df.iloc[row_itr][column_itr]\n",
    "            if isinstance(key,str):\n",
    "            #check if this key is already present in the dictionary\n",
    "                if key not in word_count_dict.keys():\n",
    "                    word_count_dict[key] = 0\n",
    "                    word_category_dict[key] = []\n",
    "            elif isinstance(key,list):\n",
    "                for key_list in key:\n",
    "                    if key_list not in word_count_dict.keys():\n",
    "                        word_count_dict[key_list] = 0  \n",
    "                        word_category_dict[key_list] = [] \n",
    "    column_labels = list(category_df.columns)\n",
    "    for key in word_count_dict.keys():\n",
    "        for column in column_labels: \n",
    "            if any(category_df[column] == key):\n",
    "                word_count_dict[key] +=1 \n",
    "                word_category_dict[key].append(column)\n",
    "    return (word_count_dict,word_category_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to get the percentage of words, classified based on emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def categoryWisePercent(frequency):\n",
    "    (word_count_dict,word_category_dict)=frequency\n",
    "    sorted_word_dict = copy.deepcopy(word_count_dict)\n",
    "    category_dict = copy.deepcopy(word_category_dict)\n",
    "    # create empty dataframe\n",
    "    percent_df = pd.DataFrame(columns = ['Category','Words','Percentage'])\n",
    "    # categorize it\n",
    "    category_list= []\n",
    "    for key in sorted_word_dict:\n",
    "        cat_list = list (category_dict[key])\n",
    "        for itr in range(0,len(cat_list)):\n",
    "            if cat_list[itr] not in category_list:\n",
    "                category_list.append(cat_list[itr])\n",
    "    percent_df['Category'] = category_list\n",
    "    percent_df['Words'] = 0\n",
    "    percent_df['Percentage'] = 0 \n",
    "    #fill words column\n",
    "    for key in sorted_word_dict:\n",
    "        cat_list = list(category_dict[key])\n",
    "        for itr in range(0,len(cat_list)):\n",
    "            index = list(np.where(percent_df['Category'] == cat_list[itr])[0])\n",
    "            percent_df.at[index,'Words'] = percent_df['Words'][index] +1\n",
    "    # fill percentage column\n",
    "    total_word_count=0\n",
    "    for row_count in range(0,len(percent_df)):\n",
    "        total_word_count += percent_df['Words'][row_count]\n",
    "    for row_count in range(0,len(percent_df)):\n",
    "        percent_df.at[row_count,'Percentage'] = ( percent_df['Words'][row_count] / total_word_count) * 100\n",
    "    #plot_bargraph(percent_df)\n",
    "    return percent_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to generate the wordcloud "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_wordcloud(wholestring,filename):\n",
    "    wordfreq = {}\n",
    "    for raw_word in wholestring:\n",
    "        word = raw_word.strip(\" \")\n",
    "        if word not in wordfreq:\n",
    "            wordfreq[word] = 0 \n",
    "        wordfreq[word] += 1\n",
    "\n",
    "    wordcloud= WordCloud(stopwords=STOPWORDS,width=1600, height=800, background_color = 'white', max_words=200, colormap=\"Dark2\",mask=image_mask)\n",
    "    wordcloud.generate_from_frequencies(frequencies=wordfreq,max_font_size=1600)\n",
    "    plt.figure(figsize=(20,10), facecolor='k')\n",
    "    plt.imshow(wordcloud.recolor(color_func=grey_color_func, random_state=3))\n",
    "    plt.imshow(image_mask, alpha=0.3)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout(pad=0)\n",
    "    plt.savefig(filename+'.png', facecolor='k', bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to show the catogerised emotions in a bar graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_bargraph(percent_df,filename):\n",
    "    per_df = copy.deepcopy(percent_df)\n",
    "    max_value= max(list(per_df['Words']))\n",
    "    index = list(np.where(per_df['Words'] == max_value)[0])\n",
    "    explode_list = []\n",
    "    for itr in range(0,len(per_df)):\n",
    "        if itr in index:\n",
    "            explode_list.append(0.1)\n",
    "        else:\n",
    "            explode_list.append(0.0)        \n",
    "    #print(explode_list)\n",
    "    explode_tuple = tuple(explode_list) \n",
    "    labels = list(per_df['Category'])\n",
    "    values = list(per_df['Percentage'])\n",
    "    \n",
    "    \n",
    "    y_pos = np.arange(len(labels))\n",
    "    axes = pie_plot.gca()\n",
    "    axes.set_ylim([0,100])\n",
    "    colors=[\"#79d279\",\"#ff4d4d\",\"#99ccff\"]\n",
    "    pie_plot.bar(y_pos, values, align='center', alpha=0.5,color=colors)\n",
    "  \n",
    "    pie_plot.xticks(y_pos, labels)\n",
    "    pie_plot.ylabel('Percentage')\n",
    "    pie_plot.title('Emotions')\n",
    "    plt.savefig(filename+'_bar.png')\n",
    "    pie_plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to genertate the scatter plot based on the polarity and location of the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_scatterplot(tempdf,name):\n",
    "    map_options = GMapOptions( map_type=\"terrain\",zoom=1)\n",
    "\n",
    "    plot = GMapPlot(\n",
    "        x_range=Range1d(), y_range=Range1d(), map_options=map_options\n",
    "    )\n",
    "    plot.title.text = \"Hey look! It's a scatter plot on a map!\"\n",
    "\n",
    "    # For GMaps to function, Google requires you obtain and enable an API key:\n",
    "    #\n",
    "    #     https://developers.google.com/maps/documentation/javascript/get-api-key\n",
    "    #\n",
    "    # Replace the value below with your personal API key:\n",
    "    plot.api_key = \"AIzaSyDpIUAZYB_QZ_ePte4xa27HJzFtZYd0tVk\"\n",
    "    colorvalue=[]\n",
    "    for cat in tempdf.category.tolist():\n",
    "        if(cat==\"positive\"):\n",
    "            colorvalue.append(\"#0000cc\")\n",
    "        else:\n",
    "            colorvalue.append(\"#e60000\")\n",
    "\n",
    "    source = ColumnDataSource(\n",
    "        data=dict(\n",
    "            lat=tempdf.latitude.tolist(),\n",
    "            lon=tempdf.longitude.tolist(),\n",
    "            color=colorvalue\n",
    "        )\n",
    "    )\n",
    "    circle = Circle(x=\"lon\", y=\"lat\",size=6, fill_color={'field': 'color'}, fill_alpha=0.5, line_color=None)\n",
    "    plot.add_glyph(source, circle)\n",
    "    plot.add_tools(PanTool(), WheelZoomTool(), BoxZoomTool())\n",
    "    #output_file(\"gmap_plot.html\")\n",
    "    output_notebook()\n",
    "    export_png(plot, filename=name+'_map.png')\n",
    "    show(plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to call the previously defined functions for data collection and data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mainprocess(hashtag):\n",
    "    dataprocess(str(hashtag.value))\n",
    "    Analysis(str(hashtag.value)+'withloc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
